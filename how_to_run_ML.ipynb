{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute mean spectrum of a Dataset\n",
    "\n",
    "\n",
    "from scipy import interpolate\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "def peak_picking(files,max_peaks):\n",
    "    \n",
    "    ints_n = 0\n",
    "    \n",
    "    min_mz = 0\n",
    "    max_mz = 0 \n",
    "    \n",
    "    for f in files:\n",
    "        ms = np.load(f)\n",
    "        mzs = ms[0,]\n",
    "        \n",
    "        if min_mz > np.min(mzs):\n",
    "            min_mz = np.min(mzs)\n",
    "            \n",
    "        if max_mz < np.max(mzs):\n",
    "            max_mz = np.max(mzs)    \n",
    "    \n",
    "    for f in files:\n",
    "        ms = np.load(f)\n",
    "\n",
    "        mzs = ms[0,]\n",
    "        ints = ms[1,]\n",
    "\n",
    "        mzs_n = np.arange(min_mz,max_mz, 0.0001)\n",
    "        \n",
    "        f = interpolate.interp1d(mzs , ints,fill_value=\"extrapolate\")\n",
    "        ints_n = ints_n + f(mzs_n)\n",
    "    \n",
    "    peaks, _ = find_peaks(ints_n,distance=100)\n",
    "    instensity_mean = ints_n[peaks]\n",
    "    mz_peaks = mzs_n[peaks]\n",
    "\n",
    "    peaks = peaks[np.argsort(ints_n[peaks])[::-1][:max_peaks]]\n",
    "\n",
    "    return(np.sort(mzs_n[peaks]))\n",
    "\n",
    "mass = peak_picking([\"MSIa.mean.npy\",\"MSIb.mean.npy\"])\n",
    "np.savetxt(\"./DS/MSI/MSIa_MSIb.peaks\", mass, delimiter=\",\")\n",
    "\n",
    "# create datacube\n",
    "#nohup python create_msi_da.py -i \"MSIa.imzML\" -i2 \"./DS/MSI/MSIa_MSIb.peaks\" -tol 0.01 -o1 \"./DS/MSIa\" &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute datacube of a dataset\n",
    "\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def check_mass_database(experimental_mass, database_mass, tolerance):\n",
    "    if database_mass != 0:\n",
    "        return abs((experimental_mass - database_mass)) <= tolerance\n",
    "    \n",
    "    \n",
    "def binarySearch_tol(arr, l, r, x, tolerance): \n",
    "  \n",
    "    while l <= r: \n",
    "  \n",
    "        mid = l + (r - l)//2; \n",
    "          \n",
    "        # Check if x is present at mid \n",
    "        if check_mass_database(x,arr[mid],tolerance): \n",
    "            # check the mass around \n",
    "            itpos = mid +1\n",
    "            itneg = mid -1\n",
    "            index = []\n",
    "            index.append(mid)\n",
    "            if( itpos < len(arr)):\n",
    "                while check_mass_database(x,arr[itpos],tolerance) and itpos < len(arr):\n",
    "                    index.append(itpos)\n",
    "                    itpos += 1 \n",
    "            if( itneg > 0): \n",
    "                while check_mass_database(x,arr[itneg],tolerance) and itneg > 0:\n",
    "                    index.append(itneg)\n",
    "                    itneg -= 1     \n",
    "            return index \n",
    "  \n",
    "        # If x is greater, ignore left half \n",
    "        elif arr[mid] < x: \n",
    "            l = mid + 1\n",
    "  \n",
    "        # If x is smaller, ignore right half \n",
    "        else: \n",
    "            r = mid - 1\n",
    "      \n",
    "    # If we reach here, then the element \n",
    "    # was not present \n",
    "    return -1\n",
    "\n",
    "def align_peaks(peaks_mz,intensities,database_exactmass, tolerance): \n",
    "    pixel_new_vec = np.zeros(np.size(database_exactmass,0))\n",
    "    for i in range(0,np.size(peaks_mz,0)):\n",
    "        exp_peak = peaks_mz[i]\n",
    "        # append the maximum mass possible to avoid going over\n",
    "        vec_ind = binarySearch_tol(np.append(database_exactmass,np.max(database_exactmass)+1), 0, len(database_exactmass)-1, exp_peak,tolerance)\n",
    "        if vec_ind != -1:\n",
    "            if len(vec_ind) > 1:\n",
    "                for j in range(0,np.size(vec_ind,0)):\n",
    "                    if intensities[i] > pixel_new_vec[vec_ind[j]]:\n",
    "                        pixel_new_vec[vec_ind[j]] = intensities[i]\n",
    "                        \n",
    "            if len(vec_ind) == 1:\n",
    "                if intensities[i] > pixel_new_vec[vec_ind]:\n",
    "                    pixel_new_vec[vec_ind] = intensities[i]\n",
    "                \n",
    "    return(pixel_new_vec)\n",
    "\n",
    "def create_datacube(file,peaks_file,df,tolerance):\n",
    "    \n",
    "    msi_mz = genfromtxt(peaks_file, delimiter=' ')\n",
    "    database_exactmass = np.sort(msi_mz)\n",
    "    \n",
    "    msi = np.zeros((np.size(database_exactmass),np.shape(df)[0]))\n",
    "\n",
    "    i = 0 \n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        print(str(row['MSI pixel id']),row['train'])\n",
    "        path = file + row['MSI name'] +\"/spec_\" + str(row['MSI pixel id']) + \".npy\"\n",
    "        spec = np.load(path)\n",
    "        \n",
    "        mode = row['train']\n",
    "        \n",
    "        mzs = spec[:,0]\n",
    "        intensities = spec[:,2]\n",
    "        \n",
    "        peaks_mz = mzs\n",
    "        vec_int = intensities\n",
    "        pixel_new_vec = align_peaks(peaks_mz,vec_int,database_exactmass, tolerance)\n",
    "        msi[:,i] = pixel_new_vec\n",
    "        i += 1\n",
    "    \n",
    "    \n",
    "    return(msi)\n",
    "\n",
    "\n",
    "peaks_file = \"./DS/MSI/MSIa_MSIb.peaks\n",
    "\n",
    "\n",
    "tolerance = 0.001\n",
    "df = pd.read_csv(\"./DS/Annot_table.csv\",sep=\",\", header=0)\n",
    "\n",
    "f = \"./DS/MSI/centroid_data/param_BGFG1/\"\n",
    "\n",
    "msi = create_datacube(f,peaks_file,df,tolerance)\n",
    "np.save(f+\"msi\",msi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "\n",
    "def rf_classifier(X, y, random_state=0):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    arr = np.arange(0,np.shape(y)[0],1)\n",
    "    np.random.shuffle(arr)\n",
    "    \n",
    "    y = y[arr]\n",
    "    X = X[arr,:]\n",
    "\n",
    "\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X, y)\n",
    "        \n",
    "    return model\n",
    "    \n",
    "def svm_classifier(X, y, random_state=0):\n",
    "    from sklearn.svm import SVC\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    arr = np.arange(0,np.shape(y)[0],1)\n",
    "    np.random.shuffle(arr)\n",
    "    \n",
    "    y = y[arr]\n",
    "    X = X[arr,:]\n",
    "\n",
    "\n",
    "    model = SVC(kernel = \"rbf\",random_state=random_state)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    return model\n",
    "    \n",
    "    \n",
    "def lda_classifier(X, y, random_state=0):\n",
    "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    arr = np.arange(0,np.shape(y)[0],1)\n",
    "    np.random.shuffle(arr)\n",
    "    \n",
    "    y = y[arr]\n",
    "    X = X[arr,:]\n",
    "\n",
    "    model = LinearDiscriminantAnalysis()\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def xgboost_classifier(X, y, random_state=0):\n",
    "    import xgboost\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    arr = np.arange(0,np.shape(y)[0],1)\n",
    "    np.random.shuffle(arr)\n",
    "    \n",
    "    y = y[arr]\n",
    "    X = X[arr,:]\n",
    "\n",
    "    model = xgboost.XGBClassifier(n_jobs=8)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def normalize(msi):\n",
    "    \n",
    "    normalised_msi = msi.copy()\n",
    "    \n",
    "    for i in range(0,np.shape(normalised_msi)[1]):\n",
    "        normalised_msi[:,i] = msi[:,i]/np.sum(msi[:,i])\n",
    "        \n",
    "    normalised_msi[np.isnan(normalised_msi)] = 0\n",
    "        \n",
    "    return normalised_msi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_annot = pd.read_csv(\"./DS/Annot_table.csv\",sep=\",\", header=0)\n",
    "results_rf = []\n",
    "results_lda = []\n",
    "results_svm = []\n",
    "results_xgboost = []\n",
    "\n",
    "f = \"./DS/MSI/centroid_data/param_BGFG1/msi.npy\"\n",
    "        \n",
    "msi = np.load(f )\n",
    "msi = normalize(np.log(msi+1))\n",
    "     \n",
    "msi = msi.astype(\"float32\")\n",
    "        \n",
    "for random_state in np.arange(0,15):\n",
    "    \n",
    "    model = rf_classifier(np.transpose(msi[:,df_annot[\"train\"].to_numpy() == 1]), df_annot.loc[df_annot[\"train\"] == 1,\"Annotations\"].to_numpy(),random_state)\n",
    "            \n",
    "    y_pred_test = model.predict(np.transpose(msi[:,df_annot[\"train\"].to_numpy() == 0]))\n",
    "    balanced_accuracy = balanced_accuracy_score(df_annot.loc[df_annot[\"train\"] == 0,\"Annotations\"],y_pred_test)\n",
    "        \n",
    "    print(balanced_accuracy)\n",
    "\n",
    "    results_rf.append(balanced_accuracy)\n",
    "            \n",
    "for random_state in np.arange(0,3):\n",
    "\n",
    "    model = lda_classifier(np.transpose(msi[:,df_annot[\"train\"].to_numpy() == 1]), df_annot.loc[df_annot[\"train\"] == 1,\"Annotations\"].to_numpy(),random_state)\n",
    "            \n",
    "    y_pred_test = model.predict(np.transpose(msi[:,df_annot[\"train\"].to_numpy() == 0]))\n",
    "    balanced_accuracy = balanced_accuracy_score(df_annot.loc[df_annot[\"train\"] == 0,\"Annotations\"],y_pred_test)\n",
    " \n",
    "    print(balanced_accuracy)\n",
    "\n",
    "    results_lda.append(balanced_accuracy)\n",
    "                                   \n",
    "for random_state in np.arange(0,3):\n",
    "    \n",
    "    model = svm_classifier(np.transpose(msi[:,df_annot[\"train\"].to_numpy() == 1]), df_annot.loc[df_annot[\"train\"] == 1,\"Annotations\"].to_numpy(),random_state)\n",
    "            \n",
    "    y_pred_test = model.predict(np.transpose(msi[:,df_annot[\"train\"].to_numpy() == 0]))\n",
    "    balanced_accuracy = balanced_accuracy_score(df_annot.loc[df_annot[\"train\"] == 0,\"Annotations\"],y_pred_test)\n",
    "\n",
    "    print(balanced_accuracy)\n",
    "\n",
    "    results_svm.append(balanced_accuracy)\n",
    "\n",
    "                                   \n",
    "for random_state in np.arange(0,3):\n",
    "    \n",
    "    model = xgboost_classifier(np.transpose(msi[:,df_annot[\"train\"].to_numpy() == 1]), df_annot.loc[df_annot[\"train\"] == 1,\"Annotations\"].to_numpy(),random_state)\n",
    "            \n",
    "    y_pred_test = model.predict(np.transpose(msi[:,df_annot[\"train\"].to_numpy() == 0]))\n",
    "    balanced_accuracy = balanced_accuracy_score(df_annot.loc[df_annot[\"train\"] == 0,\"Annotations\"],y_pred_test)\n",
    "\n",
    "        \n",
    "    print(balanced_accuracy)\n",
    "\n",
    "    results_xgboost.append(balanced_accuracy)\n",
    "        \n",
    "            \n",
    "np.savetxt(\"./DS/results_rf_SpaceMDS_NOsigdeg.csv\", results_rf , delimiter=',')\n",
    "np.savetxt(\"./DS/results_lda_SpaceMDS_NOsigdeg.csv\", results_lda, delimiter=',')\n",
    "np.savetxt(\"./DS/results_svm_SpaceMDS_NOsigdeg.csv\", results_svm, delimiter=',')\n",
    "np.savetxt(\"./DS/results_xgboost_SpaceMDS_NOsigdeg.csv\", results_xgboost, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
