{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.utils import to_categorical, np_utils\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Flatten, Conv1D, Dropout, BatchNormalization, MaxPooling1D, LeakyReLU\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, balanced_accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "# dataloader\n",
    "def bin_ms(spec,mass_range,tol):\n",
    "    \n",
    "    vec_bin = np.arange(mass_range[0],mass_range[1],tol)\n",
    "    bin_spec = np.zeros((len(vec_bin)))\n",
    "    \n",
    "    for i in range(0,len(vec_bin)):\n",
    "        bin_i = vec_bin[i]\n",
    "        index = (spec[:,0]  >= bin_i) & (spec[:,0]  < (bin_i+tol))\n",
    "        bin_spec[i] = np.sum(spec[index ,2])\n",
    "    \n",
    "    return bin_spec\n",
    "\n",
    "def load_spectra(file,mass_range,tol,df):\n",
    "\n",
    "    mass_vec = np.arange(mass_range[0],mass_range[1],tol)\n",
    "\n",
    "    mat_bin_vec = np.zeros((np.shape(df)[0],len(mass_vec)))\n",
    "    labes_vec = []\n",
    "\n",
    "    ct = 0\n",
    "    for index, row in df.iterrows():\n",
    "        #print(index, row['MSI name'], row['MSI pixel id'])\n",
    "        #labes_vec.append(row['Annotations 2'])\n",
    "        path = file + row['MSI name'] +\"/spec_\" + str(row['MSI pixel id']) + \".npy\"\n",
    "\n",
    "        spec = np.load(path)\n",
    "        bin_spec = bin_ms(spec,mass_range,tol)\n",
    "        mat_bin_vec[ct,:] = bin_spec\n",
    "        print(ct)\n",
    "        ct = ct +1\n",
    "    mat_bin_vec = np.transpose(mat_bin_vec)\n",
    "    return(mat_bin_vec)\n",
    "\n",
    "\n",
    "## Model variant_Lecun (model 1 : 4 layers)\n",
    "def build_model_lecun( ms_input_shape):\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=6, kernel_size=21, strides=1, padding='same', activation='relu', input_shape= ms_input_shape,\n",
    "               kernel_initializer=keras.initializers.he_normal()),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2, strides=2, padding='same'),\n",
    "        Conv1D(filters=16, kernel_size=5, strides=1, padding='same',activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2, strides=2, padding='same'),\n",
    "        Flatten(),\n",
    "        Dense(120, activation='relu'),\n",
    "        Dense(84),\n",
    "        Dense(nb_classes, activation='sigmoid') # or Activation('softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "## Model variant_LeNet (model 2: 5 layers)\n",
    "def build_model_lenet():\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=16, kernel_size=21, strides=1, padding='same', input_shape= ms_input_shape,\n",
    "               kernel_initializer=keras.initializers.he_normal()),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(),\n",
    "        MaxPooling1D(pool_size=2, strides=2, padding='same'),\n",
    "        Conv1D(filters=32, kernel_size=11, strides=1, padding='same'),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(),\n",
    "        MaxPooling1D(pool_size=2, strides=2, padding='same'),\n",
    "        Conv1D(filters=64, kernel_size=5, strides=1, padding='same'),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(),\n",
    "        MaxPooling1D(pool_size=2, strides=2, padding='same'),\n",
    "        Flatten(),\n",
    "        Dense(2050, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(nb_classes, activation='sigmoid') # or Activation('softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def train_cnn(mat_bin_vec,sub_df,nb_classes,tol,random_state):\n",
    "    \n",
    "    \n",
    "    #scaler = StandardScaler()\n",
    "    #scaler.fit(np.log(mat_bin_vec+1))\n",
    "    #mat_data  = scaler.transform(np.log(mat_bin_vec+1))\n",
    "\n",
    "    mat_data = minmax_scale(np.log(mat_bin_vec+1), axis=0, feature_range=(0, 1))\n",
    "    mat_data = mat_data.astype(\"float32\")\n",
    "    data = mat_data\n",
    "    y_train = sub_df.loc[sub_df['train'] ==True,\"Annotations\"]\n",
    "    y_train_conv = to_categorical(y_train,num_classes=nb_classes)\n",
    "    x_train = np.transpose(data[:,sub_df['train'] ==True])\n",
    "    x_train_conv = np.reshape(x_train,(x_train.shape[0], x_train.shape[1], 1))\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    arr = np.arange(0,np.shape(y_train_conv)[0],1)\n",
    "    np.random.shuffle(arr)\n",
    "    \n",
    "    y_train_conv = y_train_conv[arr,:]\n",
    "    x_train_conv = x_train_conv[arr,:,:]\n",
    "\n",
    "\n",
    "    # model building\n",
    "    ms_input_shape = (len(np.arange(mass_range[0],mass_range[1],tol)),    1)\n",
    "\n",
    "\n",
    "    model = build_model_lecun(ms_input_shape)\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False),metrics=['accuracy']) # or categorical_crossentropy\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, min_lr=0.0000001)\n",
    "    earlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=0, mode='auto')\n",
    "\n",
    "    # model training\n",
    "    history = model.fit(x=x_train_conv, y=y_train_conv, batch_size=254, verbose=0, epochs=50, validation_split=0.2, callbacks=[earlyStopping, reduce_lr])\n",
    "\n",
    "    return(model)\n",
    "\n",
    "def evaluate(model,sub_df,mat_bin_vec,nb_classes):\n",
    "\n",
    "    # evaluation\n",
    "\n",
    "    mat_data = minmax_scale(np.log(mat_bin_vec+1), axis=0, feature_range=(0, 1))\n",
    "    \n",
    "    #scaler = StandardScaler()\n",
    "    #scaler.fit(np.log(mat_bin_vec+1))\n",
    "    #mat_data  = scaler.transform(np.log(mat_bin_vec+1))\n",
    "    \n",
    "    mat_data = mat_data.astype(\"float32\")\n",
    "\n",
    "    data = mat_data\n",
    "\n",
    "    y_test = sub_df.loc[sub_df['train'] ==False,\"Annotations\"]\n",
    "    y_test_conv = to_categorical(y_test,num_classes=nb_classes)\n",
    "    x_test = np.transpose(data[:,sub_df['train'] ==False])\n",
    "    x_test_conv = np.reshape(x_test,(x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "    loss, acc = model.evaluate(x_test_conv, y_test_conv, verbose=0)\n",
    "    print('final accuracy: tol ' +str(tol) +\" modellecun \" + \"sub\"+str(0)+\" accuracy\",acc)\n",
    "    \n",
    "    \n",
    "    y_classes = model.predict_classes(x_test_conv, verbose=0)\n",
    "    y_prob = model.predict_proba(x_test_conv, verbose=0)\n",
    "    matrix = confusion_matrix(y_test, y_classes)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, y_classes)\n",
    "    print(matrix)\n",
    "    \n",
    "    print(\"accuracy: \",accuracy_score(y_test, y_classes))\n",
    "    print(\"recall_score: \",recall_score(y_test, y_classes))\n",
    "    print(\"precision_score: \",precision_score(y_test, y_classes))\n",
    "    print(\"balanced_accuracy: \",balanced_accuracy)\n",
    "    \n",
    "    \n",
    "    return y_prob,y_classes,balanced_accuracy\n",
    "\n",
    "def make_mat_image(im,crd):\n",
    "    mat = np.zeros((np.max(crd[:,0]),np.max(crd[:,1])))\n",
    "    for i in range(0,np.size(crd,0)):\n",
    "        mat[crd[i,0]-1,crd[i,1]-1] = im[i]\n",
    "    return mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate pre-processing data for 1D-CNN\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# dataloader\n",
    "def bin_ms(spec,mass_range,tol):\n",
    "    \n",
    "    vec_bin = np.arange(mass_range[0],mass_range[1],tol)\n",
    "    bin_spec = np.zeros((len(vec_bin)))\n",
    "    \n",
    "    for i in range(0,len(vec_bin)):\n",
    "        bin_i = vec_bin[i]\n",
    "        index = (spec[:,0]  >= bin_i) & (spec[:,0]  < (bin_i+tol))\n",
    "        bin_spec[i] = np.sum(spec[index ,2])\n",
    "    \n",
    "    return bin_spec\n",
    "\n",
    "def load_spectra(file,mass_range,tol,df):\n",
    "\n",
    "    mass_vec = np.arange(mass_range[0],mass_range[1],tol)\n",
    "\n",
    "    mat_bin_vec = np.zeros((np.shape(df)[0],len(mass_vec)))\n",
    "    labes_vec = []\n",
    "\n",
    "    ct = 0\n",
    "    for index, row in df.iterrows():\n",
    "        #print(index, row['MSI name'], row['MSI pixel id'])\n",
    "        #labes_vec.append(row['Annotations 2'])\n",
    "        path = file + row['MSI name'] +\"/spec_\" + str(row['MSI pixel id']) + \".npy\"\n",
    "\n",
    "        spec = np.load(path)\n",
    "        bin_spec = bin_ms(spec,mass_range,tol)\n",
    "        mat_bin_vec[ct,:] = bin_spec\n",
    "        print(ct)\n",
    "        ct = ct +1\n",
    "    mat_bin_vec = np.transpose(mat_bin_vec)\n",
    "    return(mat_bin_vec)\n",
    "\n",
    "\n",
    "# to change according to the sample\n",
    "mass_range = [200,1400]\n",
    "tol = 0.1\n",
    "nb_classes = 2\n",
    "\n",
    "df = pd.read_csv(\"./DS/Annot_table.csv\",sep=\",\", header=0)\n",
    "\n",
    "\n",
    "file = \"./DS/MSI/centroid_data/param_BGFG1/\"\n",
    "mat_bin_vec= load_spectra(file,mass_range,tol,df)\n",
    "np.save(file+\"mat_bin_vec\", mat_bin_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 1D-CNN\n",
    "\n",
    "mass_range = [200,1100]\n",
    "tol = 0.1\n",
    "nb_classes = 2\n",
    "\n",
    "df = pd.read_csv(\"./DS/Annot_table.csv\",sep=\",\", header=0)\n",
    "results = []\n",
    "\n",
    "f = \"./DS/MSI/centroid_data/param_BGFG1/mat_bin_vec.npy\"\n",
    "mat_bin_vec = np.load(f)\n",
    "\n",
    "for random_state in np.arange(0,15):\n",
    "\n",
    "    model = train_cnn(mat_bin_vec,df,nb_classes,tol,random_state)\n",
    "    y_prob,y_classes,accuracy = evaluate(model,df,mat_bin_vec,nb_classes)\n",
    "    print(accuracy)\n",
    "    results.append(accuracy)\n",
    "\n",
    "np.savetxt(\"./DS/results_cnn_SpaceMDS_NOsigdeg.csv\", results, delimiter=',')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
